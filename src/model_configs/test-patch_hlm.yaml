
model_type: patch_hlm
architectures: [
    PatchHLmModel
]

max_sequence_length: 16

hidden_size: 128
mlp_size: 256

attention_head_size: 32
num_attention_heads: 3
num_iaf_attention_heads: 2
use_register: true

num_layers: 4
num_decoder_layers: 2

hidden_act: silu
norm_eps: 0.00001

rope_fraction: 2
rope_base: 10000

z_size: 11
z_mlp_mult: 5

z_output_layers: 3

patch_size: 4

gradient_checkpointing: false
